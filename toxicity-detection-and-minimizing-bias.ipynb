{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phase 1","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:32.146677Z","iopub.execute_input":"2021-06-24T16:55:32.147038Z","iopub.status.idle":"2021-06-24T16:55:32.151092Z","shell.execute_reply.started":"2021-06-24T16:55:32.146993Z","shell.execute_reply":"2021-06-24T16:55:32.15022Z"}}},{"cell_type":"markdown","source":"## Libraries and Packages","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:32.152561Z","iopub.execute_input":"2021-06-24T16:55:32.153099Z","iopub.status.idle":"2021-06-24T16:55:32.163734Z","shell.execute_reply.started":"2021-06-24T16:55:32.153063Z","shell.execute_reply":"2021-06-24T16:55:32.162888Z"}}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt     \nimport seaborn as sns\n\nimport string\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D,GlobalAveragePooling1D\nfrom keras.layers import Bidirectional, SpatialDropout1D, GRU\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom keras.utils import to_categorical\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.optimizers import RMSprop, Adam\n\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\n\nimport gc\n## warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-06-25T04:54:04.249738Z","iopub.execute_input":"2021-06-25T04:54:04.250142Z","iopub.status.idle":"2021-06-25T04:54:11.651294Z","shell.execute_reply.started":"2021-06-25T04:54:04.250056Z","shell.execute_reply":"2021-06-25T04:54:11.650378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Data","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:32.17985Z","iopub.execute_input":"2021-06-24T16:55:32.1801Z","iopub.status.idle":"2021-06-24T16:55:32.186742Z","shell.execute_reply.started":"2021-06-24T16:55:32.180076Z","shell.execute_reply":"2021-06-24T16:55:32.185946Z"}}},{"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:54:11.652697Z","iopub.execute_input":"2021-06-25T04:54:11.653039Z","iopub.status.idle":"2021-06-25T04:54:28.889524Z","shell.execute_reply.started":"2021-06-25T04:54:11.653004Z","shell.execute_reply":"2021-06-25T04:54:28.88854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:43.17944Z","iopub.execute_input":"2021-06-24T16:55:43.179716Z","iopub.status.idle":"2021-06-24T16:55:43.185748Z","shell.execute_reply.started":"2021-06-24T16:55:43.179689Z","shell.execute_reply":"2021-06-24T16:55:43.18251Z"}}},{"cell_type":"code","source":"# Missing values\ntrain.isnull().sum()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-25T04:54:28.891515Z","iopub.execute_input":"2021-06-25T04:54:28.891883Z","iopub.status.idle":"2021-06-25T04:54:29.569802Z","shell.execute_reply.started":"2021-06-25T04:54:28.891845Z","shell.execute_reply":"2021-06-25T04:54:29.568379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, '\\n')\ntrain.info()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-25T04:54:29.571723Z","iopub.execute_input":"2021-06-25T04:54:29.572221Z","iopub.status.idle":"2021-06-25T04:54:29.592024Z","shell.execute_reply.started":"2021-06-25T04:54:29.572184Z","shell.execute_reply":"2021-06-25T04:54:29.591191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:54:29.593439Z","iopub.execute_input":"2021-06-25T04:54:29.59394Z","iopub.status.idle":"2021-06-25T04:54:29.621161Z","shell.execute_reply.started":"2021-06-25T04:54:29.593896Z","shell.execute_reply":"2021-06-25T04:54:29.62023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing Non-null Target distribuition","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:43.917169Z","iopub.execute_input":"2021-06-24T16:55:43.917645Z","iopub.status.idle":"2021-06-24T16:55:43.9241Z","shell.execute_reply.started":"2021-06-24T16:55:43.917593Z","shell.execute_reply":"2021-06-24T16:55:43.92333Z"}}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\ngraph_1 = sns.distplot(train[train['target'] > 0]['target'], color = 'red')\nplt.title('Toxicity (Target) Distribution')\nplt.xlabel(\"Toxicity Rate\")\nplt.ylabel(\"Distribution\") \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-25T04:54:29.622575Z","iopub.execute_input":"2021-06-25T04:54:29.62291Z","iopub.status.idle":"2021-06-25T04:54:32.309528Z","shell.execute_reply.started":"2021-06-25T04:54:29.622874Z","shell.execute_reply":"2021-06-25T04:54:32.308666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Non-null Sub-classes Distribution","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:46.838054Z","iopub.execute_input":"2021-06-24T16:55:46.838398Z","iopub.status.idle":"2021-06-24T16:55:46.842147Z","shell.execute_reply.started":"2021-06-24T16:55:46.838362Z","shell.execute_reply":"2021-06-24T16:55:46.841156Z"}}},{"cell_type":"code","source":"comment_adjective = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n\nplt.figure(figsize=(15,6))\n\nfor col in comment_adjective:\n    graph_2 = sns.distplot(train[train[col] > 0][col], label=col, hist=False)\n    plt.xlabel(\"Rate\", fontsize=16)\n    plt.ylabel(\"Distribution\", fontsize=16)\n    plt.legend(loc=1, prop={'size': 14})\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-25T04:54:32.310874Z","iopub.execute_input":"2021-06-25T04:54:32.311209Z","iopub.status.idle":"2021-06-25T04:54:38.104743Z","shell.execute_reply.started":"2021-06-25T04:54:32.311173Z","shell.execute_reply":"2021-06-25T04:54:38.103901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Phase 2","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.336397Z","iopub.execute_input":"2021-06-24T16:55:52.336654Z","iopub.status.idle":"2021-06-24T16:55:52.340286Z","shell.execute_reply.started":"2021-06-24T16:55:52.336628Z","shell.execute_reply":"2021-06-24T16:55:52.339297Z"}}},{"cell_type":"markdown","source":"## Data Pre-processing","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.342623Z","iopub.execute_input":"2021-06-24T16:55:52.342977Z","iopub.status.idle":"2021-06-24T16:55:52.352766Z","shell.execute_reply.started":"2021-06-24T16:55:52.342942Z","shell.execute_reply":"2021-06-24T16:55:52.351958Z"}}},{"cell_type":"markdown","source":"### Embeddings\n\nGloVe pre-trained word vectors: https://nlp.stanford.edu/projects/glove/\n\n6B tokens, 100d vectors","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.362754Z","iopub.execute_input":"2021-06-24T16:55:52.362997Z","iopub.status.idle":"2021-06-24T16:55:52.377465Z","shell.execute_reply.started":"2021-06-24T16:55:52.362974Z","shell.execute_reply":"2021-06-24T16:55:52.374628Z"}}},{"cell_type":"markdown","source":"### Creating Vocabulary","metadata":{}},{"cell_type":"code","source":"\"\"\" GLOVE_EMBEDDING_PATH = \"./glove.6B.100d.txt\"\n\nimport pickle\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\nglove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)\nprint('Found and loaded {} word vectors'.format(len(glove_embeddings)))\n\n#!rm \"./glove.840B.300d.pkl\"\n\n\"check_coverage\" goes through a given vocabulary and tries to find word vectors in embedding matrix. \"build_vocab\" builds a ordered dictionary of words and their frequency in the text corpus.\n\nimport operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\" \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\" \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\nvocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]\n\n### Symbols in GloVe\n\nimport string\nletter_digit_list = string.ascii_letters + string.digits + ' '\nletter_digit_list += \"'\"\n\nSymbols that have embedding vectors in GloVe:\n\nglove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\nglove_symbols = ''.join([c for c in glove_chars if not c in letter_digit_list])\nglove_symbols\n\nSymbols that have no embedding vectors in GloVe:\n\njigsaw_chars = build_vocab(list(train[\"comment_text\"]))\n\njigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in letter_digit_list])\njigsaw_symbols\n\nsymbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])\nsymbols_to_delete\n\nsymbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])\nsymbols_to_isolate\n\ndel glove_embeddings\ndel vocab\ndel glove_chars\ndel glove_symbols\n\ngc.collect() \"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:54:38.107218Z","iopub.execute_input":"2021-06-25T04:54:38.107559Z","iopub.status.idle":"2021-06-25T04:54:38.115222Z","shell.execute_reply.started":"2021-06-25T04:54:38.107524Z","shell.execute_reply":"2021-06-25T04:54:38.114221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from nltk.tokenize.treebank import TreebankWordTokenizer\n#tokenizer = TreebankWordTokenizer()\n\n\n#isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n#remove_dict = {ord(c):f'' for c in symbols_to_delete}\n\ndef cleaning_text(x):\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    x = clean_special_chars(x, punct)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = cleaning_text(x)\n    x = fix_quote(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:54:38.117376Z","iopub.execute_input":"2021-06-25T04:54:38.117926Z","iopub.status.idle":"2021-06-25T04:54:38.126231Z","shell.execute_reply.started":"2021-06-25T04:54:38.11789Z","shell.execute_reply":"2021-06-25T04:54:38.1253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure all comment_text values are strings\ntrain['comment_text'] = train['comment_text'].astype(str) \n\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# Convert target and identity columns to booleans\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + identity_columns:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\ntrain = convert_dataframe_to_bool(train)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:54:38.127668Z","iopub.execute_input":"2021-06-25T04:54:38.128005Z","iopub.status.idle":"2021-06-25T04:54:40.189913Z","shell.execute_reply.started":"2021-06-25T04:54:38.12797Z","shell.execute_reply":"2021-06-25T04:54:40.189029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['comment_text'] = train['comment_text'].progress_apply(lambda x:preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:54:40.191295Z","iopub.execute_input":"2021-06-25T04:54:40.191621Z","iopub.status.idle":"2021-06-25T04:56:27.032836Z","shell.execute_reply.started":"2021-06-25T04:54:40.191584Z","shell.execute_reply":"2021-06-25T04:56:27.031721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, validate_df = model_selection.train_test_split(train, test_size=0.2)\nprint('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:56:27.034334Z","iopub.execute_input":"2021-06-25T04:56:27.034691Z","iopub.status.idle":"2021-06-25T04:56:28.593634Z","shell.execute_reply.started":"2021-06-25T04:56:27.034654Z","shell.execute_reply":"2021-06-25T04:56:28.592529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_NUM_WORDS = 10000\nTARGET_COLUMN = 'target'\nTEXT_COLUMN = 'comment_text'","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:56:28.595186Z","iopub.execute_input":"2021-06-25T04:56:28.595572Z","iopub.status.idle":"2021-06-25T04:56:28.599879Z","shell.execute_reply.started":"2021-06-25T04:56:28.595535Z","shell.execute_reply":"2021-06-25T04:56:28.598802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Padding Function","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.401187Z","iopub.status.idle":"2021-06-24T16:55:52.401979Z"}}},{"cell_type":"code","source":"# Create a text tokenizer.\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(train[TEXT_COLUMN])\n\n# All comments must be truncated or padded to be the same length.\nMAX_SEQUENCE_LENGTH = 250\ndef pad_text(texts, tokenizer):\n    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:56:28.60122Z","iopub.execute_input":"2021-06-25T04:56:28.601604Z","iopub.status.idle":"2021-06-25T04:59:00.300264Z","shell.execute_reply.started":"2021-06-25T04:56:28.601567Z","shell.execute_reply":"2021-06-25T04:59:00.299378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ================ Phase 3 ================","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.407025Z","iopub.status.idle":"2021-06-24T16:55:52.408051Z"}}},{"cell_type":"markdown","source":"# Pre-training Dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.409193Z","iopub.status.idle":"2021-06-24T16:55:52.409978Z"}}},{"cell_type":"code","source":"\"\"\"\n!kaggle datasets download -d fizzbuzz/cleaned-toxic-comments\n\n!unzip \"./cleaned-toxic-comments.zip\"\n\npre_data = \"./train_preprocessed.csv\"\n\n\n## Get the Corpus of all the comments and related Toxicity fields\n\ndata = pd.read_csv(pre_data)\ndata.head()\n\n## Dividing the dataset into features and labels:\nFeatures = \"comment\"            \nLabels = \"toxicity\"\n\nFeatures = data['comment_text']\nLabels = np.array([0 if y == 0 else 1 for y in data['toxicity']])\n\n### Tokenizing and preprocessing the data\n\nNUM_WORDS = 40000 # Maximum number of unique words which need to be tokenized\nMAXLEN = 50 # Maximum length of a sentence/ comment\nPADDING = 'post' # The type of padding done for sentences shorter than the Max len\n\ntokenizer = Tokenizer(num_words=NUM_WORDS)\n\n# Fit the tokenizer on the comments \ntokenizer.fit_on_texts(Features)\n\n# Get the word index of the top 20000 words from the dataset\nword_idx = tokenizer.word_index\n\n# Convert the string sentence to a sequence of their numerical values\nFeature_sequences = tokenizer.texts_to_sequences(Features)\n\n# Pad the sequences to make them of uniform length\npadded_sequences = pad_sequences(Feature_sequences, maxlen = MAXLEN, padding = PADDING)\n\nprint(\"The Transformation of sentence::\")\nprint(\"\\n\\nThe normal Sentencen:\\n\")\nprint(Features[2])\nprint(\"\\n\\nThe tokenized sequence:\\n\")\nprint(Feature_sequences[2])\nprint(\"\\n\\nThe padded sequence:\\n\")\nprint(padded_sequences[2])\n\n# Convert to array for passing through the model\nX = np.array(padded_sequences)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:59:00.301638Z","iopub.execute_input":"2021-06-25T04:59:00.301966Z","iopub.status.idle":"2021-06-25T04:59:00.308998Z","shell.execute_reply.started":"2021-06-25T04:59:00.301932Z","shell.execute_reply":"2021-06-25T04:59:00.308097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ================ Phase 4 ==================","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.413127Z","iopub.status.idle":"2021-06-24T16:55:52.413922Z"}}},{"cell_type":"markdown","source":"# Training","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.415082Z","iopub.status.idle":"2021-06-24T16:55:52.415875Z"}}},{"cell_type":"markdown","source":"### Hyper-parameters","metadata":{}},{"cell_type":"code","source":"EMBEDDINGS_PATH = '../input/glove-6b-100d/glove.6B.100d.txt'\nEMBEDDINGS_DIMENSION = 100\nLEARNING_RATE = 0.0005\nNUM_EPOCHS = 5\nBATCH_SIZE = 128\n\n# Prepare data\ntrain_text = pad_text(train_df[TEXT_COLUMN], tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T04:59:00.310401Z","iopub.execute_input":"2021-06-25T04:59:00.310745Z","iopub.status.idle":"2021-06-25T05:02:01.948786Z","shell.execute_reply.started":"2021-06-25T04:59:00.310711Z","shell.execute_reply":"2021-06-25T05:02:01.947904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label = to_categorical(train_df[TARGET_COLUMN])","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:01.950223Z","iopub.execute_input":"2021-06-25T05:02:01.950574Z","iopub.status.idle":"2021-06-25T05:02:01.972185Z","shell.execute_reply.started":"2021-06-25T05:02:01.95054Z","shell.execute_reply":"2021-06-25T05:02:01.97137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:01.973405Z","iopub.execute_input":"2021-06-25T05:02:01.973759Z","iopub.status.idle":"2021-06-25T05:02:02.206125Z","shell.execute_reply.started":"2021-06-25T05:02:01.973721Z","shell.execute_reply":"2021-06-25T05:02:02.205149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test=train_test_split(train_text, train_label, test_size=0.20, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:02.207597Z","iopub.execute_input":"2021-06-25T05:02:02.207942Z","iopub.status.idle":"2021-06-25T05:02:02.699921Z","shell.execute_reply.started":"2021-06-25T05:02:02.207904Z","shell.execute_reply":"2021-06-25T05:02:02.699089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:02.701194Z","iopub.execute_input":"2021-06-25T05:02:02.701561Z","iopub.status.idle":"2021-06-25T05:02:02.885131Z","shell.execute_reply.started":"2021-06-25T05:02:02.701526Z","shell.execute_reply":"2021-06-25T05:02:02.884155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Embedding Matrix","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.426898Z","iopub.status.idle":"2021-06-24T16:55:52.42769Z"}}},{"cell_type":"code","source":"# Load embeddings\nprint('loading embeddings')\nembeddings_index = {}\nwith open(EMBEDDINGS_PATH) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n                                 EMBEDDINGS_DIMENSION))\nnum_words_in_embedding = 0\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        num_words_in_embedding += 1\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\nprint(\"Embeddings loaded!\")","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:02.886561Z","iopub.execute_input":"2021-06-25T05:02:02.887216Z","iopub.status.idle":"2021-06-25T05:02:18.158394Z","shell.execute_reply.started":"2021-06-25T05:02:02.887169Z","shell.execute_reply":"2021-06-25T05:02:18.156891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create model layers.\nmodel=Sequential()\nmodel.add(Embedding(len(tokenizer.word_index) + 1,100,input_length = 100,weights = [embedding_matrix],trainable = False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(512,activation = 'relu'))\nmodel.add(Dense(512,activation = 'relu'))\nmodel.add(Dense(2,activation='softmax'))\n\n# Compile model.\nprint('Compiling model...')\nmodel.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(lr=LEARNING_RATE),\n                  metrics=['acc'])\nprint(\"Compiled model!\")","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:18.159608Z","iopub.execute_input":"2021-06-25T05:02:18.159934Z","iopub.status.idle":"2021-06-25T05:02:20.588806Z","shell.execute_reply.started":"2021-06-25T05:02:18.159906Z","shell.execute_reply":"2021-06-25T05:02:20.587968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:20.592415Z","iopub.execute_input":"2021-06-25T05:02:20.592661Z","iopub.status.idle":"2021-06-25T05:02:20.60436Z","shell.execute_reply.started":"2021-06-25T05:02:20.592636Z","shell.execute_reply":"2021-06-25T05:02:20.603492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-training on external data","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.434948Z","iopub.status.idle":"2021-06-24T16:55:52.435746Z"}}},{"cell_type":"code","source":"\"\"\"\nhistory = model.fit(\n            X, \n            Labels,\n            batch_size = 128,\n            epochs = 10,\n            validation_split = 0.2, # 20 percent data reserved for validation to avoid or monitor overfitting/ underfitting\n            verbose = verbose,\n        )\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:20.60628Z","iopub.execute_input":"2021-06-25T05:02:20.606672Z","iopub.status.idle":"2021-06-25T05:02:20.613363Z","shell.execute_reply.started":"2021-06-25T05:02:20.606637Z","shell.execute_reply":"2021-06-25T05:02:20.612343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Training on competition data","metadata":{"execution":{"iopub.status.busy":"2021-06-24T16:55:52.438907Z","iopub.status.idle":"2021-06-24T16:55:52.43971Z"}}},{"cell_type":"code","source":"model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"best_lstm_toxic.h5\",\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:20.615069Z","iopub.execute_input":"2021-06-25T05:02:20.615611Z","iopub.status.idle":"2021-06-25T05:02:20.621015Z","shell.execute_reply.started":"2021-06-25T05:02:20.615576Z","shell.execute_reply":"2021-06-25T05:02:20.620091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:20.622511Z","iopub.execute_input":"2021-06-25T05:02:20.622999Z","iopub.status.idle":"2021-06-25T05:02:20.628377Z","shell.execute_reply.started":"2021-06-25T05:02:20.622965Z","shell.execute_reply":"2021-06-25T05:02:20.627607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model.\nprint('Training model...')\nstart = time.time()\nhistory = model.fit(X_train,\n              y_train,\n              batch_size=BATCH_SIZE,\n              epochs= NUM_EPOCHS,\n              validation_data=(X_test, y_test),\n              verbose=1, callbacks = [model_checkpoint_callback])\n\nend = time.time()\nprint(\"Training duration: {} minutes\".format(str((end-start)/60)))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T05:02:20.629737Z","iopub.execute_input":"2021-06-25T05:02:20.630115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = 'lstm_model'\nvalidate_df[MODEL_NAME] = model.predict(pad_text(validate_df[TEXT_COLUMN], tokenizer))[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\nbias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TARGET_COLUMN)\nbias_metrics_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_overall_auc(df, model_name):\n    true_labels = df[TARGET_COLUMN]\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n    \nget_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train, y_train, X_test, y_test\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.save('../toxicity_classifier.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsubmission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))[:, 1]\nsubmission.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}